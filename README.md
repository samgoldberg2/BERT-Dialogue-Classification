# Introduction

We intend to use a text classification model in order to better understand frequently occurring patterns in human dialogue. Specifically, in our project we are looking to identify commonly used dialogues types and to learn about the chronological order in which these types are most likely to be used. This analysis will then be used to indicate certain descriptors of dialogue. For instance, the comparison between frequency of questions and frequency of commands may predict politeness. Further, if a question is repeatedly being followed up by statements, this would predict a conversational component. This use of text classification is important because of the ways in which it can be applied. One way we believe our model can be utilized is in helping to teach people who lack social skills about appropriate ways to communicate. A quantified and physical display of language patterns is a rarely used methodology for teaching about this and may provide insight into a new perspective. We hope to create an accurate classification model that can help achieve this if given any text containing dialogue.

# Hypothesis

We expect our model to conclude that in our corpus, statements are the most commonly used type of dialogue, while emotional expressions are the least. This is because of the fact that the dialogue in our data comes from professional meetings. Such meetings are commonly used for one party to share information rather than for asking questions or giving commands. Further, being in a professional environment, these meetings usually preclude emotion and emotional expressions therefore should be the most limited in quantity. We also expect that the most frequent dialogue type to follow questions in regard to chronological order will be statements. This is because the questions should be followed by answers in the form of a statement since the questions used in meetings are unlikely to be rhetorical given that it is not a lecture. Finally, we predict that commands most often follow emotional expressions because if emotion does become involved in the meeting, we believe it would be met with some sort of course of action to eliminate whatever is causing it.

# Data

Our data comes from the Meeting Recorder Dialog Act (MRDA) corpus, which we sourced using a Github link. It contains 75 hours of speech from 75 real meetings transcripts with 53 different speakers. The data was prelabeled into 61 classes, which we then mapped onto 5 different label groups. Based on the MRDA corpus, we mapped different existing labels from the corpus to our own created classes. These groups are Questions, Statements, Commands, Emotional Expressions, and Other. For the ‘question’ class, it includes all of the inquiry-based pieces of text in the corpus. It includes tags like 'qy' for Yes-No questions, 'qw' for Wh-questions (who, what, where, when, why), 'qo' for open-ended questions, and 'qr' for either/or type questions. For the ‘command’ class, it includes dialogue acts that convey a directive or a request. Tags like 'ad' (Action-directive) and 'co' (Offer) fall into this category. These represent instances where a speaker is either instructing someone to perform an action or offering something. For the ‘statement’ class, which occurs very frequently, it includes tags such as 's', which cover both Statement-non-opinion and Statement-opinion. This broad category captures declarative expressions where speakers share information, opinions, or beliefs. For the ‘emotional expression’ class, it is very important in understanding the emotional nuances in dialogue. It includes tags like 'ba' (Assessment/Appreciation), indicating a speaker’s evaluation or appreciation of something, 'aa' (Agree/Accept) for agreement expressions, 'fe' (Exclamation) for exclamatory remarks, 'by' (Sympathy), 'fa' (Apology), and responses like 'na' (Affirmative Non-Yes Answers) and 'ng' (Negative Non-No Answers). For the ‘other’ class, it kind of serves as a “catch all” for various dialogue acts that do not fit into the other groups. It includes tags like 'x' (Nonspeech) for non-verbal communication elements and 't1' (Self-talk) for instances where a speaker is talking to themselves, among others One possible limitation with this data is the professional setting in which it was recorded. Because we would like to be able to apply the analysis to all human dialogue, this may be problematic in the different ways people talk in this environment. For example, the way one communicates with a friend will largely vary from how one communicates with their boss. With friends, people are likely to speak without much thought and just say anything that comes to mind - whether it fits appropriate language conventions or not. Contrastingly, when in a meeting, people may pay more attention to the ways they speak so as to appear composed and informed. Another possible limitation is the large number of speakers involved in the meetings from which our data is derived. Because of this quantity, it is likely these speakers have a variety of different backgrounds and communication styles, which makes for some inconsistency regarding speech patterns that may impact our model’s predictions. In order to analyze our data we will use a few classification models and visualizations. We will use baseline classifiers to gain an understanding of how well our actual model is working so that we can better understand its results. For our baseline, we created a logistic regression model and a Naive Bayesian model. We chose these models because they are both very simple and work well with the categorical data involved in our dataset. For our actual model, we are using BERT. We chose this model primarily because of its strength in capturing contextual information. Regarding dialogue classification, a BERT model’s ability to understand the intent of a sentence is huge in that this type of information is essentially what decides how a piece of dialogue will function. Moreover, BERT models are trained on a very large amount of text, which gives them a unique understanding of varying language patterns. This can help to combat some of the limitations felt from the large number of different speakers who were part of the meeting data. We will then use some graphs from matplot to help visualize patterns in the data in order to allow for an even better understanding of what the model is doing beyond its f1, precision, accuracy, and support scores.

# Results

When comparing the BERT model with the logistic regression and naive bayes baseline classifiers, the BERT model showed a substantially superior performance. It achieved a weighted F1-score of 90.02%, indicating a strong balance between precision and recall across various classes. The strong performance of the model is particularly noticeable in categories like 'command' and 'statement', where BERT's precision and recall were also better than the baseline models. The Logistic Regression baseline model had an F1-score of 64% while the Naive Bayes baseline classifier had a weighted F1-score of 61%. In particular, these baseline models struggled in categories requiring deeper contextual understanding, such as 'command' and 'question', which BERT handled more effectively. When looking at the BERT predictions vs. True labels graphs, it is very evident that the BERT model was extremely accurate in predicting whether a piece of text belonged in the ‘other’ category. While I did map most of the tags to the ‘other’ category since they did not fit into the other four categories, I was still vastly impressed by the ability for BERT to classify pretty much every single occurrence of ‘other’ correctly.

The substantial difference in performance can be attributed to BERT's advanced ability to capture contextual nuances in language, in which the baseline classifiers (logistic regression and naive bayes) definitely lack in. It’s worth noting that the superior performance of the BERT model came at a significant cost in terms of computational resources and training time. The BERT model was very computationally intensive and required a very long time to train, especially since we implemented stratified K-Fold cross-validation, which involved multiple training cycles across different data subsets. I mean, our corpus was also huge! There is definitely a tradeoff with using BERT in terms of feasibility, resource allocation, and time efficiency. Overall, there is an undeniable advantage to using BERT in terms of the accuracy with text classification compared to the baseline classifiers.

The dataset comprises a total of 108,202 entries, distributed across various categories with 'other' being the most predominant (56,597 entries), followed by 'statement' (33,472 entries), 'emotional expression' (10,154 entries), 'question' (4,272 entries), and 'command' (3,707 entries). Despite the very large skew in the data, BERT was able to achieve impressive results. This also elucidates the fact of perhaps why the baseline classifiers performed so poorly. Perhaps there was not enough training data for them! The BERT model was undeniably less affected by the imbalance in the data. The performance, which can be seen from the F1 scores, shows a very strong capability to handle imbalanced datasets effectively.

As far as results in our classification, our model demonstrated the various patterns of dialogue types throughout a conversation. In order to understand the implications of these patterns better, we did some calculations to quantify dialogue descriptors like politeness and conversational factor. By comparing the number of questions to the number of commands, we were able to determine a ratio that quantified the politeness of the data. A ratio of over 1 indicates a polite conversation, being that more questions were asked than commands given, while a ratio of less than 1 indicates an aggressive conversation, since more commands were made. This ratio works because questions are often used as a kind way to get help, while commands are used when someone has no care for whether or not the person wants to help. In our data, the model had a ratio of 1.18 which suggests a relatively equal spread of commands and questions but hints to a more polite conversation overall. In order to understand how conversational the data was, we calculated the amount of times that questions were followed up with a statement in comparison to the amount of questions asked overall. A statement made after a question can easily be inferred as an answer to the question. As such, this calculation helps to show how responsive people were to questions asked during the meetings. This then provides insight to how conversational the meetings were overall. Our data had a conversational factor of .317. This is pretty low and suggests that people only answered questions 31.7% of the time.

# Conclusion

More than just being able to classify dialogue in this way, our model helped to provide some very valuable insights into the frequently occurring patterns in our data. We found that we were incorrect in our prediction that statements were the most occurring type, since the “other” category actually was double the amount of statements. This was an oversight in our hypothesis because this category is non specific to any type and just exists to group leftover data that had no significance in our classification task. It is as such that its large size makes sense, however being the most frequently occurring type is not a really valuable insight. Still, we were not far off about statements, being that after the “other” category it was the next most prevalent. Surprisingly, emotional expressions were next in quantity with about a third of the amount of statements. This was followed by questions, and then commands. We take this to mean that these meetings did have a larger emotional component than we assumed. We are curious if the meetings in our data are more emotional than the average meeting, or if we underestimated the amount that emotion plays into professional work. Because of the huge amount of dialogue in the “other” category, when comparing the chronological order of types we decided to omit this category, since it does not have much value. When examining the first 100 pieces of dialogue, we found that we were correct in our prediction that questions were most commonly followed by statements. We were incorrect in our prediction that emotional expression would most commonly be followed by commands. In fact, every emotional expression within the first 100 pieces of dialogue was followed by a statement. We believe that this suggests people responded to the emotion in a respectful, conversational manner. We were also surprised to see that all of the commands in this subset of data were followed by questions. This is interesting since this pattern suggests a pretty non conversational way of communicating. To answer a question with a command is not productive, which proves again a limitation in using this type of professional conversation to apply to other situations.

Problems we ran into Throughout our work, we ran into a couple computational challenges. One instance came towards the beginning of our efforts. Once we built our logistic regression model, we noticed our data was only being classified into 3 groups (question, statement, and other) rather than our five. We were able to solve this by debugging our code with print statements. We realized that we were pulling the wrong tags from the corpus because of a mistake in our indexing. Each piece of dialogue had four types of tags, and we were using the third tag rather than the fourth one - which is the one that held the type. Once we reconciled this we were able to sufficiently run our first model.

Another challenge we encountered was the limitation imposed by Google Colab's GPU access. This issue was primarily observed while training the BERT model, which as I said before, is very computationally and resource intensive. In Google Colab, a popup appeared stating “You cannot currently connect to a GPU due to usage limits in Colab.” After doing some reading, it seems like Google Colab offers free access to GPUs, but it is certainly subject to usage limits to ensure fair allocation among a vast number of users. With the high computational demands of training the BERT model, especially with stratified K-Fold cross-validation, and the large corpus, it's no wonder I hit these limits. Training the BERT model required a great deal of processing power, which depleted the GPUs available in a session of a free Google Colab account.

Despite these obstacles, we feel that we successfully built a classification model that groups dialogue into five groups: Questions, Statements, Commands, Emotional Expressions, and Other. As mentioned previously, we feel a really meaningful application of our model would be to help people who struggle with social cues regarding conversational dialogue. There are many ways this model can be implemented in order to help with this, like as a tool to learn about the common uses of dialogue from each class. Beyond this, our model may be used to serve a variety of purposes regarding business meeting statics and summaries. Text classification is a really interesting task and has so many possible functions. We are eager to see the ways in which it can be used as a learning tool in the future.

